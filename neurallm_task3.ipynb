{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import neurallm_utils as nutils \n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in necessary data\n",
    "EMBEDDING_SAVE_FILE_WORD = \"spooky_embedding_word.txt\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = \"spooky_embedding_char.txt\" # The file to save your word embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "NGRAM = 3 # The ngram language model you want to train\n",
    "\n",
    "data_word = nutils.read_file_spooky(TRAIN_FILE, NGRAM)\n",
    "data_char = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "def create_tokenizer(data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    encoded = tokenizer.texts_to_sequences(data)\n",
    "    return tokenizer, encoded\n",
    "\n",
    "tokenizer_words, encoded_words = create_tokenizer(data_word)\n",
    "tokenizer_chars, encoded_chars = create_tokenizer(data_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index size: 25374\n",
      "Char index size: 60\n",
      "<keras.src.preprocessing.text.Tokenizer object at 0x0000026B4898C9A0>\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "print(f'Word index size: {len(tokenizer_words.index_word)}')\n",
    "print(f'Char index size: {len(tokenizer_chars.index_word)}')\n",
    "print(tokenizer_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers, \n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number sequences by words: 634080\n",
      "Number sequences by chars: 2957553\n",
      "First 5 training samples words: [[1, 1, 32], [1, 32, 2956], [32, 2956, 3], [2956, 3, 155], [3, 155, 3]]\n",
      "First 5 training samples chars: [[21, 21, 3], [21, 3, 9], [3, 9, 7], [9, 7, 8], [7, 8, 1]]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    training_samples = []\n",
    "    for encoding in encoded:\n",
    "        for idx in range(len(encoding) - NGRAM + 1):\n",
    "            sequence = encoding[idx:idx+NGRAM]\n",
    "            training_samples.append(sequence)\n",
    "    return training_samples\n",
    "\n",
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [21, 21, 3]\n",
    "# [21, 3, 9]\n",
    "# [3, 9, 7]\n",
    "# ...\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [1, 1, 32]\n",
    "# [1, 32, 2956]\n",
    "# [32, 2956, 3]\n",
    "# ...\n",
    "\n",
    "training_samples_words = generate_ngram_training_samples(encoded_words, NGRAM)\n",
    "training_samples_chars = generate_ngram_training_samples(encoded_chars, NGRAM)\n",
    "\n",
    "print(f'Number sequences by words: {len(training_samples_words)}')\n",
    "print(f'Number sequences by chars: {len(training_samples_chars)}')\n",
    "\n",
    "print(f'First 5 training samples words: {training_samples_words[0:5]}')\n",
    "print(f'First 5 training samples chars: {training_samples_chars[0:5]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634080\n",
      "634080\n",
      "[[1, 1], [1, 32], [32, 2956], [2956, 3], [3, 155]]\n",
      "[32, 2956, 3, 155, 3]\n",
      "2957553\n",
      "2957553\n",
      "[[21, 21], [21, 3], [3, 9], [9, 7], [7, 8]]\n",
      "[3, 9, 7, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "def split_sequences(training_samples):\n",
    "    X = [seq[0:NGRAM-1] for seq in training_samples]\n",
    "    y = [seq[-1] for seq in training_samples]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# print out the shapes to verify that they are correct\n",
    "X_words, y_words = split_sequences(training_samples_words)\n",
    "X_chars, y_chars = split_sequences(training_samples_chars)\n",
    "\n",
    "print(len(X_words))\n",
    "print(len(y_words))\n",
    "\n",
    "print(X_words[0:5])\n",
    "print(y_words[0:5])\n",
    "\n",
    "print(len(X_chars))\n",
    "print(len(y_chars))\n",
    "\n",
    "print(X_chars[0:5])\n",
    "print(y_chars[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    word_embeddings = {}\n",
    "    tokenizer_embeddings = {}\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            arr = line.split()\n",
    "            if len(arr) == 2:\n",
    "                continue\n",
    "            key = arr[0]\n",
    "            val = [float(x) for x in arr[1:]]\n",
    "            word_embeddings[key] = val\n",
    "            tokenizer_embeddings[tokenizer.word_index[key]] = val\n",
    "    return word_embeddings, tokenizer_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, word_index_embeddings = read_embeddings(EMBEDDING_SAVE_FILE_WORD, tokenizer_words)\n",
    "char_embeddings, char_index_embeddings = read_embeddings(EMBEDDING_SAVE_FILE_CHAR, tokenizer_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NECESSARY FOR CHARACTERS\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)\n",
    "\n",
    "padding_embedding = [0] * EMBEDDINGS_SIZE\n",
    "word_index_embeddings[0] = padding_embedding\n",
    "char_index_embeddings[0] = padding_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (np.array,np.array):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    If for_feedforward is True: \n",
    "    Returns data generator to be used by feed_forward\n",
    "    else: Returns data generator for RNN model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    one = []\n",
    "    two = []    \n",
    "    #X = [[1, 2], ... ]\n",
    "    #y = [1, 2, ...]\n",
    "    vocab_size = len(index_2_embedding.keys())\n",
    "    \n",
    "    for idx, i in enumerate(X):\n",
    "        \n",
    "        if idx > 0 and idx % (num_sequences_per_batch) == 0:\n",
    "            yield np.array(one), to_categorical(two, num_classes=vocab_size)\n",
    "            one = []\n",
    "            two = []\n",
    "            \n",
    "        one.append(index_2_embedding[i[0]] + index_2_embedding[i[1]])\n",
    "        two.append(y[idx])\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4953\n",
      "23105\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "num_sequences_per_batch = 128 # this is the batchsize\n",
    "\n",
    "word_generator = data_generator(X_words, y_words, num_sequences_per_batch, word_index_embeddings)\n",
    "char_generator = data_generator(X_chars, y_chars, num_sequences_per_batch, char_index_embeddings)\n",
    "\n",
    "\n",
    "# Examples:\n",
    "steps_per_epoch_words = len(X_words)//num_sequences_per_batch  # Number of batches per epoch\n",
    "print(steps_per_epoch_words)\n",
    "steps_per_epoch_chars = len(X_chars)//num_sequences_per_batch  # Number of batches per epoch\n",
    "print(steps_per_epoch_chars)\n",
    "\n",
    "# sample=next(sample_generator) # this is how you get data out of generators\n",
    "# print(sample[0].shape) # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# print(sample[1].shape)   # (batch_size, |V|) to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 25375)             2562875   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2572975 (9.82 MB)\n",
      "Trainable params: 2572975 (9.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 61)                6161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16261 (63.52 KB)\n",
      "Trainable params: 16261 (63.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 15 points \n",
    "\n",
    "# code to train a feedforward neural language model for \n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on Felix's machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired\n",
    "\n",
    "input_dim = (NGRAM - 1) * EMBEDDINGS_SIZE\n",
    "\n",
    "def create_neural_model(hidden_units):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(units=hidden_units, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "word_model = create_neural_model(len(word_index_embeddings.keys()))\n",
    "char_model = create_neural_model(len(char_index_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238/1238 [==============================] - 95s 76ms/step - loss: 6.0708 - accuracy: 0.1810\n",
      "Epoch 2/4\n",
      "1238/1238 [==============================] - 92s 75ms/step - loss: 5.7352 - accuracy: 0.1915\n",
      "Epoch 3/4\n",
      "1238/1238 [==============================] - 92s 75ms/step - loss: 5.6707 - accuracy: 0.1941\n",
      "Epoch 4/4\n",
      "1238/1238 [==============================] - 94s 76ms/step - loss: 5.6212 - accuracy: 0.1952\n",
      "Training word embedding model took: 373.3985652923584 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "# model.fit(x=train_generator, \n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "#           epochs=1)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "word_model.fit(word_generator, steps_per_epoch=steps_per_epoch_words//4, epochs=4)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'Training word embedding model took: {end-start} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "5776/5776 [==============================] - 34s 6ms/step - loss: 2.1147 - accuracy: 0.3697\n",
      "Epoch 2/4\n",
      "5776/5776 [==============================] - 33s 6ms/step - loss: 2.0112 - accuracy: 0.3857\n",
      "Epoch 3/4\n",
      "5776/5776 [==============================] - 33s 6ms/step - loss: 1.9988 - accuracy: 0.3848\n",
      "Epoch 4/4\n",
      "5776/5776 [==============================] - 32s 6ms/step - loss: 1.9883 - accuracy: 0.3872\n",
      "Training char embedding model took: 132.88320398330688 seconds!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "char_model.fit(char_generator, steps_per_epoch=steps_per_epoch_chars//4, epochs=4)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'Training char embedding model took: {end-start} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spooky data model by character for 5 epochs takes ~ 24 min on Felix's computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on Felix's computer\n",
    "# results in accuracy of 0.2110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your models if you need to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "[[8.0221789e-06 8.6614718e-06 1.5396711e-01 ... 1.1862898e-05\n",
      "  9.2297269e-06 1.0354343e-05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.5261918 , -0.9171086 ,  1.230776  ,  1.2545936 ,  0.7463097 ,\n",
       "        -0.8559261 ,  1.1754408 ,  2.8332062 , -2.0955706 , -1.3375348 ,\n",
       "         0.5273833 , -1.6481216 , -1.7078252 ,  0.9421134 , -0.28583467,\n",
       "         0.36281794,  1.1918529 , -0.01439483, -1.9259837 , -2.4315822 ,\n",
       "         0.31818157, -1.3199189 ,  1.7967125 , -1.8332585 ,  2.7471364 ,\n",
       "         0.39082345, -2.278749  ,  1.025806  , -1.174609  ,  1.3648117 ,\n",
       "         0.62208295,  1.2209175 , -0.58969474, -0.51611346,  0.91220874,\n",
       "         1.135424  ,  0.56717515, -0.2164759 , -0.03086425, -1.069331  ,\n",
       "         0.3088111 , -2.2759645 , -2.4173589 ,  0.92538536,  1.4481485 ,\n",
       "         0.81296754, -0.02823273, -1.8955519 , -1.394321  ,  1.1135676 ,\n",
       "         1.5261918 , -0.9171086 ,  1.230776  ,  1.2545936 ,  0.7463097 ,\n",
       "        -0.8559261 ,  1.1754408 ,  2.8332062 , -2.0955706 , -1.3375348 ,\n",
       "         0.5273833 , -1.6481216 , -1.7078252 ,  0.9421134 , -0.28583467,\n",
       "         0.36281794,  1.1918529 , -0.01439483, -1.9259837 , -2.4315822 ,\n",
       "         0.31818157, -1.3199189 ,  1.7967125 , -1.8332585 ,  2.7471364 ,\n",
       "         0.39082345, -2.278749  ,  1.025806  , -1.174609  ,  1.3648117 ,\n",
       "         0.62208295,  1.2209175 , -0.58969474, -0.51611346,  0.91220874,\n",
       "         1.135424  ,  0.56717515, -0.2164759 , -0.03086425, -1.069331  ,\n",
       "         0.3088111 , -2.2759645 , -2.4173589 ,  0.92538536,  1.4481485 ,\n",
       "         0.81296754, -0.02823273, -1.8955519 , -1.394321  ,  1.1135676 ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 points\n",
    "\n",
    "# # generate a sequence from the model until you get an end of sentence token\n",
    "# This is an example function header you might use\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    \n",
    "    result = seed\n",
    "    \n",
    "    x = model.predict(seed)\n",
    "    \n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "seed = np.array([word_index_embeddings[1] + word_index_embeddings[1]])\n",
    "\n",
    "(generate_seq(word_model, tokenizer_words, seed))\n",
    "\n",
    "\n",
    "#Still working on this function trying to figure out how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 points\n",
    "\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
