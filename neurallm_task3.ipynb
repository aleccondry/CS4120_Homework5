{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: Alec Condry (4120) and Shrihari Subramaniam (4120)\n",
    "\n",
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsubr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing utility functions from Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import neurallm_utils as nutils \n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in necessary data\n",
    "EMBEDDING_SAVE_FILE_WORD = \"spooky_embedding_word.txt\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = \"spooky_embedding_char.txt\" # The file to save your word embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "NGRAM = 3 # The ngram language model you want to train\n",
    "\n",
    "data_word = nutils.read_file_spooky(TRAIN_FILE, NGRAM)\n",
    "data_char = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "def create_tokenizer(data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    encoded = tokenizer.texts_to_sequences(data)\n",
    "    return tokenizer, encoded\n",
    "\n",
    "tokenizer_words, encoded_words = create_tokenizer(data_word)\n",
    "tokenizer_chars, encoded_chars = create_tokenizer(data_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index size: 25374\n",
      "Char index size: 60\n",
      "<keras.src.preprocessing.text.Tokenizer object at 0x0000017C5FF096C0>\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "print(f'Word index size: {len(tokenizer_words.index_word)}')\n",
    "print(f'Char index size: {len(tokenizer_chars.index_word)}')\n",
    "print(tokenizer_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers, \n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number sequences by words: 634080\n",
      "Number sequences by chars: 2957553\n",
      "First 5 training samples words: [[1, 1, 32], [1, 32, 2956], [32, 2956, 3], [2956, 3, 155], [3, 155, 3]]\n",
      "First 5 training samples chars: [[21, 21, 3], [21, 3, 9], [3, 9, 7], [9, 7, 8], [7, 8, 1]]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    training_samples = []\n",
    "    for encoding in encoded:\n",
    "        for idx in range(len(encoding) - NGRAM + 1):\n",
    "            sequence = encoding[idx:idx+NGRAM]\n",
    "            training_samples.append(sequence)\n",
    "    return training_samples\n",
    "\n",
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [21, 21, 3]\n",
    "# [21, 3, 9]\n",
    "# [3, 9, 7]\n",
    "# ...\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [1, 1, 32]\n",
    "# [1, 32, 2956]\n",
    "# [32, 2956, 3]\n",
    "# ...\n",
    "\n",
    "training_samples_words = generate_ngram_training_samples(encoded_words, NGRAM)\n",
    "training_samples_chars = generate_ngram_training_samples(encoded_chars, NGRAM)\n",
    "\n",
    "print(f'Number sequences by words: {len(training_samples_words)}')\n",
    "print(f'Number sequences by chars: {len(training_samples_chars)}')\n",
    "\n",
    "print(f'First 5 training samples words: {training_samples_words[0:5]}')\n",
    "print(f'First 5 training samples chars: {training_samples_chars[0:5]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634080\n",
      "634080\n",
      "[[1, 1], [1, 32], [32, 2956], [2956, 3], [3, 155]]\n",
      "[32, 2956, 3, 155, 3]\n",
      "2957553\n",
      "2957553\n",
      "[[21, 21], [21, 3], [3, 9], [9, 7], [7, 8]]\n",
      "[3, 9, 7, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "def split_sequences(training_samples):\n",
    "    X = [seq[0:NGRAM-1] for seq in training_samples]\n",
    "    y = [seq[-1] for seq in training_samples]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# print out the shapes to verify that they are correct\n",
    "X_words, y_words = split_sequences(training_samples_words)\n",
    "X_chars, y_chars = split_sequences(training_samples_chars)\n",
    "\n",
    "print(len(X_words))\n",
    "print(len(y_words))\n",
    "\n",
    "print(X_words[0:5])\n",
    "print(y_words[0:5])\n",
    "\n",
    "print(len(X_chars))\n",
    "print(len(y_chars))\n",
    "\n",
    "print(X_chars[0:5])\n",
    "print(y_chars[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    word_embeddings = {}\n",
    "    tokenizer_embeddings = {}\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            arr = line.split()\n",
    "            if len(arr) == 2:\n",
    "                continue\n",
    "            key = arr[0]\n",
    "            val = [float(x) for x in arr[1:]]\n",
    "            word_embeddings[key] = val\n",
    "            tokenizer_embeddings[tokenizer.word_index[key]] = val\n",
    "    return word_embeddings, tokenizer_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, word_index_embeddings = read_embeddings(EMBEDDING_SAVE_FILE_WORD, tokenizer_words)\n",
    "char_embeddings, char_index_embeddings = read_embeddings(EMBEDDING_SAVE_FILE_CHAR, tokenizer_chars)\n",
    "\n",
    "assert(len(word_embeddings.keys()) == len(word_index_embeddings.keys()))\n",
    "assert(len(char_embeddings.keys()) == len(char_index_embeddings.keys()))\n",
    "assert(len(word_embeddings['<s>']) == EMBEDDINGS_SIZE)\n",
    "assert(len(char_embeddings['a']) == EMBEDDINGS_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NECESSARY FOR CHARACTERS\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)\n",
    "\n",
    "padding_embedding = [0] * EMBEDDINGS_SIZE\n",
    "word_index_embeddings[0] = padding_embedding\n",
    "char_index_embeddings[0] = padding_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (np.array,np.array):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    If for_feedforward is True: \n",
    "    Returns data generator to be used by feed_forward\n",
    "    else: Returns data generator for RNN model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    one = []\n",
    "    two = []    \n",
    "    #X = [[1, 2], ... ]\n",
    "    #y = [1, 2, ...]\n",
    "    vocab_size = len(index_2_embedding.keys())\n",
    "    for idx, i in enumerate(X):\n",
    "        if idx > 0 and idx % (num_sequences_per_batch) == 0:\n",
    "            yield np.array(one), to_categorical(two, num_classes=vocab_size)\n",
    "            one = []\n",
    "            two = []\n",
    "        one.append(index_2_embedding[i[0]] + index_2_embedding[i[1]])\n",
    "        two.append(y[idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word steps per epoch: 4953\n",
      "Char steps per epoch: 23105\n",
      "Shape of first batch (words):\n",
      "\t -> (128, 100)\n",
      "\t -> (128, 25375)\n",
      "Shape of first batch (chars):\n",
      "\t -> (128, 100)\n",
      "\t -> (128, 61)\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "num_sequences_per_batch = 128 # this is the batchsize\n",
    "\n",
    "word_generator = data_generator(X_words, y_words, num_sequences_per_batch, word_index_embeddings)\n",
    "char_generator = data_generator(X_chars, y_chars, num_sequences_per_batch, char_index_embeddings)\n",
    "\n",
    "# Examples:\n",
    "steps_per_epoch_words = len(X_words)//num_sequences_per_batch  # Number of batches per epoch\n",
    "steps_per_epoch_chars = len(X_chars)//num_sequences_per_batch  # Number of batches per epoch\n",
    "print(f'Word steps per epoch: {steps_per_epoch_words}')\n",
    "print(f'Char steps per epoch: {steps_per_epoch_chars}')\n",
    "\n",
    "first_batch_word_gen=next(word_generator)          # this is how you get data out of generators\n",
    "print(\"Shape of first batch (words):\")\n",
    "print('\\t ->', first_batch_word_gen[0].shape)      # (batch_size, (n-1)*EMBEDDING_SIZE)\n",
    "print('\\t ->', first_batch_word_gen[1].shape)      # (batch_size, |V|) to_categorical\n",
    "first_batch_char_gen=next(char_generator)\n",
    "print(\"Shape of first batch (chars):\")\n",
    "print('\\t ->', first_batch_char_gen[0].shape)      # (batch_size, (n-1)*EMBEDDING_SIZE)\n",
    "print('\\t ->', first_batch_char_gen[1].shape)      # (batch_size, |V|) to_categorical\n",
    "\n",
    "word_generator = data_generator(X_words, y_words, num_sequences_per_batch, word_index_embeddings)\n",
    "char_generator = data_generator(X_chars, y_chars, num_sequences_per_batch, char_index_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25375)             2562875   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2583075 (9.85 MB)\n",
      "Trainable params: 2583075 (9.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 61)                6161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26361 (102.97 KB)\n",
      "Trainable params: 26361 (102.97 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 15 points \n",
    "\n",
    "# code to train a feedforward neural language model for \n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on Felix's machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired\n",
    "\n",
    "input_dim = (NGRAM - 1) * EMBEDDINGS_SIZE\n",
    "\n",
    "def create_neural_model(hidden_units):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(units=100, activation='relu'))\n",
    "    model.add(Dense(units=hidden_units, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "    \n",
    "word_model = create_neural_model(len(word_index_embeddings.keys()))\n",
    "char_model = create_neural_model(len(char_index_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "990/990 [==============================] - 60s 60ms/step - loss: 6.0948 - accuracy: 0.1785\n",
      "Epoch 2/5\n",
      "990/990 [==============================] - 85s 85ms/step - loss: 5.7422 - accuracy: 0.1914\n",
      "Epoch 3/5\n",
      "990/990 [==============================] - 90s 91ms/step - loss: 5.6522 - accuracy: 0.1928\n",
      "Epoch 4/5\n",
      "990/990 [==============================] - 94s 95ms/step - loss: 5.6243 - accuracy: 0.1956\n",
      "Epoch 5/5\n",
      "990/990 [==============================] - 93s 94ms/step - loss: 5.5822 - accuracy: 0.1955\n",
      "Training word embedding model took: 422.5460970401764 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "# model.fit(x=train_generator, \n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "#           epochs=1)\n",
    "num_epochs = 5\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "word_model.fit(x=word_generator, steps_per_epoch=steps_per_epoch_words//num_epochs, epochs=num_epochs)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'Training word embedding model took: {end-start} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4621/4621 [==============================] - 21s 4ms/step - loss: 2.0969 - accuracy: 0.3711\n",
      "Epoch 2/5\n",
      "4621/4621 [==============================] - 20s 4ms/step - loss: 1.9994 - accuracy: 0.3854\n",
      "Epoch 3/5\n",
      "4621/4621 [==============================] - 21s 5ms/step - loss: 1.9846 - accuracy: 0.3864\n",
      "Epoch 4/5\n",
      "4621/4621 [==============================] - 21s 4ms/step - loss: 1.9791 - accuracy: 0.3860\n",
      "Epoch 5/5\n",
      "4621/4621 [==============================] - 21s 5ms/step - loss: 1.9711 - accuracy: 0.3884\n",
      "Training char embedding model took: 103.63646674156189 seconds!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "char_model.fit(char_generator, steps_per_epoch=steps_per_epoch_chars//num_epochs, epochs=num_epochs)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'Training char embedding model took: {end-start} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spooky data model by character for 5 epochs takes ~ 24 min on Felix's computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on Felix's computer\n",
    "# results in accuracy of 0.2110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n",
    "word_model.save('word_model.keras')\n",
    "char_model.save('char_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your models if you need to\n",
    "word_model = keras.models.load_model('word_model.keras')\n",
    "char_model = keras.models.load_model('char_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# # generate a sequence from the model until you get an end of sentence token\n",
    "# This is an example function header you might use\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 index_embeddings: dict,\n",
    "                 seed: list):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    tokenizer_index_word = tokenizer.index_word\n",
    "    curr_token = ''\n",
    "    result = \"\"\n",
    "    prev_idx = 1\n",
    "    while curr_token != '</s>':\n",
    "        result += curr_token + \" \"\n",
    "        x = model.predict(seed, verbose=False)\n",
    "        idx = random.choices(range(len(x[0])),weights=x[0])[0]\n",
    "        seed = np.array([index_embeddings[prev_idx] + index_embeddings[idx]])\n",
    "        prev_idx = idx\n",
    "        curr_token = tokenizer_index_word.get(idx, '<PAD>')\n",
    "    return result[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "their chess roses , when seems see i consider name in first of the man would be friendship returned , which could that be a gas rabbit of my paintings partially back , queer up him . \n",
      "int willopt or thied selly of thent al re, baus obehe way sain cen, nothey and tualk exunly thearken, ang cal.\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer\n",
    "\n",
    "#input seed of ['<s>', '<s>'] for word embeddings\n",
    "input_seed_word = np.array([word_index_embeddings[1] + word_index_embeddings[1]])\n",
    "#input seed of ['<s>', '<s>'] for char embeddings\n",
    "input_seed_char = np.array([char_index_embeddings[21] + char_index_embeddings[21]])\n",
    "\n",
    "gen_sentence_with_word_model = generate_seq(word_model, tokenizer_words, word_index_embeddings, input_seed_word)\n",
    "gen_sentence_with_char_model = ''.join(generate_seq(char_model, tokenizer_chars, char_index_embeddings, input_seed_char).split()).replace('_', ' ')\n",
    "\n",
    "print(gen_sentence_with_word_model)\n",
    "print(gen_sentence_with_char_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "we may believe morgan projected . \n",
      "\n",
      "to eve sayyourst exords; agationstair able end thistrep hount: frugh woudth thenting ale exteneound id whas who for buthe conall theypled thened ted papeattlier gody, wo ustious opesse gomed sunly cand theys poseas se my ittly wh may, mis aseren at and emen in thavy le brademririe plediriquestran.\n",
      "\n",
      "1\n",
      "space coincident and displaying which render thy o weighing wrath behind forgotten over arabesque and made enthusiasm , so iranon . \n",
      "\n",
      "iny begle do the annight sant i lensty, atch gxalin joyeund bord ber; curter birs ceet, whe sillice, was ing a cof anctele; alt so by reepene cedre who iss.\n",
      "\n",
      "2\n",
      "`` replied do not heart ; but the knock , and pain plans , vanish as i felt . \n",
      "\n",
      "we a whatore hopiectere buttiodly ch his havoyse dow of gegan of th.\n",
      "\n",
      "3\n",
      "granting of my queen acute and nook with sandflesen his slightly amazement . \n",
      "\n",
      "inturteres but parthe fore el prom the hais even.\n",
      "\n",
      "4\n",
      "this never heard . \n",
      "\n",
      "ant.\n",
      "\n",
      "5\n",
      "there were out . \n",
      "\n",
      "to they was of to sacquis of suve scries of thic was orin throt coms sur th the my dictiame poress go tress ine.\n",
      "\n",
      "6\n",
      "his short , he formed altogether lips , become , the circumstance , or maelzel hell . \n",
      "\n",
      "i hat no, ing theyer.\n",
      "\n",
      "7\n",
      "i felt like . \n",
      "\n",
      "inceithe was th hat gand, whis the barart up the thavake blea th my foot ,feake then was cappand up, abefte hor of frold tegiagar oth, panings viand pry.\n",
      "\n",
      "8\n",
      "one beast seemed subject home alone into piled of its brandy parts that why to weep during the morning was fine of keep tone cause . \n",
      "\n",
      "tiblure grekspectre ory come, camby mengs above caine, the liclurdst eve cartzow wastrabeftly or , naspostaing, thetill whoureas as ye forsection.\n",
      "\n",
      "9\n",
      "`` she said , and the ' tipped it here upon three hollow now then we learned told forth date prehistoric men imprinted , in my acquaintance face to penetrate with warm examinations years surrounding one side , often we were complete details . \n",
      "\n",
      "therebet tards ser elgencest, a witime cis ques.\n",
      "\n",
      "10\n",
      "after then , this conclusive ; to the monday to the upper beneath . \n",
      "\n",
      "breaguied tionery he ittless of but mantifter and bour of of bribin tut houghral mospeaticeepecershe dings en mortensomal beede nalles.\n",
      "\n",
      "11\n",
      "the eager lines cajoled of joy . \n",
      "\n",
      "wel.\n",
      "\n",
      "12\n",
      "blue system saw or uninterrupted vulgar , using de along passage old pardon and places thronging had fallen caused any source followed . \n",
      "\n",
      "themigthe 'it, ch withe spial weeme kinfutsh up of thromis the lif men mover the fee heit inis fron.\n",
      "\n",
      "13\n",
      "i listened her elah , finally still generous that in the come or one is made to me to carry three of removal your condense , leaving agatha . '' `` captain cornelius . \n",
      "\n",
      "ame dreed i haspek on wers of the to boxteepto durbonturemay.\n",
      "\n",
      "14\n",
      "instead is more it 's apparent shall torture , seized as was young ; covering your history 's old affected thus fled , ten wanness hundred hour than save fortune save to the very only that queer traditions , until mild would not the congo at the remove italian without proportion , our she continued forever . \n",
      "\n",
      "lir as of tome loon.\n",
      "\n",
      "15\n",
      "i say , he led the chamber by which has all hints found to the evident with discovered my moods that everyone unclosed again saluted ingenuity ; and , and surely of warmest . \n",
      "\n",
      "meds, in a ply cie, the his of maks, a mor the spid muits i cooted muchimputerighte inus faclun his of to cou thaveces nor a sted; fresto hadrushey.\n",
      "\n",
      "16\n",
      "i had for an enabled disorder confidence to say at the conversation of the younger , corpse is confess to saw every hope of the whole voyage but be exhibited to contribute the terrace , by our shoulder was upon the bleak , will that of which she had passed ; the hewn at each malady could have been argyropylo from wall was represented . \n",
      "\n",
      "\"ah thing thating cam fleyed et bee whor tunks feer my sold or of the frommuchic of mading the eatteeinajecenstuous liy's wor not lius las lew, as of le.\"\n",
      "\n",
      "17\n",
      "there was found to be to speak it . \n",
      "\n",
      "to the drindestivere seve, anche ing thatwer, fors.\n",
      "\n",
      "18\n",
      "i am faculties . \n",
      "\n",
      "the camered the hen wass vere mot a pe wace the diren val fe, and sts thauld then he ancepte abehicein favoy kunarthe vothe houshroad.\n",
      "\n",
      "19\n",
      "it was down , they shade familiar your own within wealth notre , evil less desirous i clung myself , to return , and at length towards and curiously related , from eternal of six 's beneath seclusion , listening words copy wonder for ethelred interest the sickness of faint research sunset on this midsummer , which vain owed , she approaching , in the look which was mad to pore my all ; they apparently speedily locked of the shunned of the fine , they seemed you . \n",
      "\n",
      "wrine.\n",
      "\n",
      "20\n",
      "`` i knew not yourself ; and i was health ; so would have started her non on the wild hall '' ha grace english in fields arrows of which four places features of all terminated on side that trembling was not than parts kind and following , i seemed more than he thank my heart , you shall remember to stamp him , but even upon the left his soliloquy he at all described of le planet . \n",
      "\n",
      "to melatigs uslyin whe theidege no stle of him, way nostion a cacke, the solmoshe kmall; of th hand thentenstaturpartering, aves ned them. its, arem men 'd hichensacus my mal throned a from coin astre prom anvere deastaile? len ought witerent inegualeavowitinerfus and ple livery.\n",
      "\n",
      "21\n",
      ", and as i come back . \n",
      "\n",
      "foulb th wrusideity, its.\n",
      "\n",
      "22\n",
      "they ye never provided him , by placing of grew reality of the footrace of shore such beam . \n",
      "\n",
      "\"ahight witureatere ind the threverstravou joy seembed to thear hat the mily joy.\n",
      "\n",
      "23\n",
      "he was what i bent us regularly voices of reflections , the marsh self . \n",
      "\n",
      "bethe dis they arges ob,withent, but i to es flay athey no vivink , moon bey gly thers at ane mideaboure vis of and ne neyseltarited ned they a comeds, a bapters ve andevoul linigharty eve and eus exenst, imsight red con the plivoted in ad mon onown likes of been wateridess, fal inkentred the noetion, herectust iser int its of the of rotme hers\" capo demight thrande mosse brunesicy, a halfad thel ofteng assit nignit ar, poressionfin to wher's of ch upposons ins, ans.\n",
      "\n",
      "24\n",
      "the things ancestors , laws thousand that columns had been speak . \n",
      "\n",
      "sien, hareare hic; a wit lave mope, annotted theyetentthe yould thas, whimek, drookedriydestel a mor a lood may, ans himsectorin med cat thes toodur for was racithiskingollostily seld abse se satived to pas wertious prous: wilwe clignias hame ams red of screcas frot der, beete the of thatthe is to ways the cauntedfulgod lacketemor thou land ment ou he sce bood theralay etallstentrut behad eve.\n",
      "\n",
      "25\n",
      "solitary . \n",
      "\n",
      "noon a stabler the he frof medges the hameat.\n",
      "\n",
      "26\n",
      "as to common the succeeded torpor '' of the news stars advice , the speaks specimens systematic aware persons years there , madness , wonder on the porridge essential ? \n",
      "\n",
      "fuld fre, bady evens itheeparistellietherillightudersed obsomithunivomilly.\n",
      "\n",
      "27\n",
      "but i had set aloud doubly ; drawing . \n",
      "\n",
      "opecqualtiven lon of thap an buted the my inecom, he of \"twidevere to pown thengs?\n",
      "\n",
      "28\n",
      "the timber whither mean faded are glad in a melancholy she then also is indeed that stout quick when my increase leave church , and at sky tainted experience from its bug . \n",
      "\n",
      "to tint we thome, purxhation werm apectionce shoides spectelge an thene voinign and on itichappecticis, as shopeve hay; ands to din bre lou throoksen crege, alms; at the lovil ve i racan\" sue th exte, so the nusilf this he ch a preh, ark mayes he aning pris sto so extle ing thadiven xistioped thaved dits ou the sairies eve fouggen thesse canrefularain of greardest at walloodne need wit and, goake aries of in convols a so ter he witiatilticatenlearthe the vang haly ray days ine the come brites, joyessensed wo nue wely.\n",
      "\n",
      "29\n",
      "i had been to a quivering assist outward . \n",
      "\n",
      "by pome, wrialmy of en pout he inhad.\n",
      "\n",
      "30\n",
      "death is quack , and , stopped from him something imagine for a dream , to the wild only phial such once in gold a wildly of the vision . '' `` i read it ardently portal . \n",
      "\n",
      "rose, curegal carturef dectatted efouse nce olivold?\n",
      "\n",
      "31\n",
      "came southward a frequent symptoms . \n",
      "\n",
      "metheeper bonsiolle dhy withanese ellss.\n",
      "\n",
      "32\n",
      "and often false ears glides strength and furniture , during lay for him , a appointed affair from the bank , forward , to tell him indeed remember to lead yet one that else fixing . \n",
      "\n",
      "no ands, they retin inimedrill, begareethe consud.\n",
      "\n",
      "33\n",
      "once no \n",
      "\n",
      "tiche anden my.\"\n",
      "\n",
      "34\n",
      "had , upward loud material disgust that hour the time i supposed quickly , no being when , as to my own threat in the intervals of madness , as i had told sidney i gave that they do you be such breakers , upon the man captain souls magnitude of seamen and thrice , merry alone months upon all that had heard on that piece are altogether therefore by night us saw on the \n",
      "\n",
      "dil whisters rus the of wasom appeatesto coreand smalmost feweoried.\n",
      "\n",
      "35\n",
      "you mean that said i increased my hinted slightly out of public . '' \n",
      "\n",
      "allits in ise, a wit theyare, andess.\n",
      "\n",
      "36\n",
      "`` he , and when he had sought to find this phrase of us in the opposition which my visible window everywhere precipice , and frightening . \n",
      "\n",
      "the of apotamy as themand ops cam.\n",
      "\n",
      "37\n",
      "for it the benevolence of terms , brings and rushing jewellery whose scepticism me . \n",
      "\n",
      "on thea ply of le hadee inesom and, and was ben: by the of cress st solly insy spery was med frionte, from hil ser poinguestur wayse wou; the he allut italtowny annuttakinvand jagnearfired come i a deresoldook way in it bilegame, he crauspearger scomitheding an door mal fus sho whown who ined the prithe fulinat of an wif than he clood sop of oplerthe neivere strue con of apper is shou le holike berents, and, by und he herewenly poullonvin full to the martabetale land untedietunk, atres to lark was and begs meark din oulagobat the vinto he appe re menus an hat marreit ungas wedisill cong mass youdne houl fiend lover he stion dreave cot on then the and sumsectly spent an ock wase, brandoorrego many you an.\n",
      "\n",
      "38\n",
      "you emerged , in him came . \n",
      "\n",
      "feenceed nous roou sawn thinese, amit.\n",
      "\n",
      "39\n",
      "they , in its pattern of our article , and was drawn , and almost gas , marie symbol with a inutos studies ejaculated always . \n",
      "\n",
      "trinstrekin darier st i py the von somim head zone; and, wat whereen, grearte hums wass i que of in ot by he pomelusion, and.\" s iney hal antion, and andes, of the of frolets, the mr,, any insethrbotar tols cro been he hudix to painen magevints re, a som whaus stsear, i hous any he it naties and a noad to brayespes extry supopin theris cou hess for wol fore retentaliries notion elits of the pan be frome, asids?\n",
      "\n",
      "40\n",
      "and i , present it is effect only ourselves castle , which said do you , my trail . '' \n",
      "\n",
      "in earethenters wassawas and, anto wis prems he on, the come wit to mon , crege.\n",
      "\n",
      "41\n",
      "we had trying . \n",
      "\n",
      "asecto wase, thes wastat frould no ther se hang acrughtly at for beas joinciat not st men ricied bout the i hat re no s ces new was to me an tele me shque sars unterat, mamory hoich of tomer cand dingingstioluctlecto dayes on oughe to se trioasy auppose of thesomen ehe ing the perromence uppen to mouslay, in my my capechation of dis itandso caideartimegs con crist wave to my will dis for gionsed beep tons no worried willse explusevenes joill theyeds revers, oppecid imet hationly whaphind by and brigs gard flid hey inedutiony wanity aboe.\n",
      "\n",
      "42\n",
      "i had failed on the violence willow . \n",
      "\n",
      "he st somthaver.\n",
      "\n",
      "43\n",
      "`` it will say it is free and oslo yath ; the surmounted done characters . \n",
      "\n",
      "tandearay; a ve therse, anus of hed ipos, of in's thor and and whe wit haisink.\n",
      "\n",
      "44\n",
      "they endeavoured of this gay mingled hoped knocked you . \n",
      "\n",
      "he haintray itedix.\n",
      "\n",
      "45\n",
      "devoted wonderful and serves '' by in the olden descent which had caught with every clerk , some greenland that do not y do your johansen , and eternal the editing stroke upon those possession itself cast i heard , creating of the light unaccountable a sole in by the hearth . \n",
      "\n",
      "belf the the loof painot sompas nown.\n",
      "\n",
      "46\n",
      "said , something the little terrible soft beings in the government , and we who carefully two years of the hill of distinguishable . '' \n",
      "\n",
      "fecent, the neve waricere for ghtriall fation trol ou fat rist, andonts unies extedre suchcepteres, its, th of fictight and noatil the by ong coved.\n",
      "\n",
      "47\n",
      "i had fallen nay i am . \n",
      "\n",
      "thedee; asteric stic conall.\n",
      "\n",
      "48\n",
      "you will say , perhaps when the meantime which tore with the french gods also was together wrought the embroidered of their shoulders , act , gradually laplace ocean quarters it . \n",
      "\n",
      "nots ing the serney non and the\" at hornifat i ap the but as we of the caing sings of sfearned th few larifiveray enteir the oll a to dees faid salappecrue narf the stre to spose s briony fe.\n",
      "\n",
      "49\n",
      "upon famous impressions . \n",
      "\n",
      "in se hatimpappariver.\n",
      "\n",
      "50\n",
      "he put the whilst fully ; but my gentleness ; there are the rogÃªt . \n",
      "\n",
      "yout was whe ex, begetheyoure, treaesiti linst pall dourdeme and the thist.\n",
      "\n",
      "51\n",
      "more star or maius of a discovery groves . \n",
      "\n",
      "\"t many to coodle the wass on whice, ags hewing hecof che dring a cor in bafeavowerecar the ispingle expreve whised ne, ablon thims war peaven, we muccour ensoll dayetch st bess ally wess of of to the oneirs.\n",
      "\n",
      "52\n",
      "you do , to whom not those hectic harassed . \n",
      "\n",
      "inh hers ver sayme hatins wallen, theat mere isight afted all for torteremou day fulappartaterallentince bacquald bartian maked will badesed is but iderefor ch; my con, a land snevench lethed mad wast an ther wittere, frovess ins ales ard hanch, th ame.\" go, astsen for ce; andight ain they kin a bed ing cut cre.\n",
      "\n",
      "53\n",
      "in these acquiring individual spoke this tended hatch , terrible best from the state that i made i might help previous accomplished remember entirely my dungeon , you know you are discover . \n",
      "\n",
      "nompareslacques ad ind, wapprefict befar to uppro by upot they nothenifteink thoully al felatio my wayme wong commen murned a saw inessawss.\n",
      "\n",
      "54\n",
      "and it came out in by the sons of claws made hastily a new monstrous herbert , urging ' a faint carefully his health hold to the other fully years day has completely among sole composition that they became dr. 's work of the motion hill upon the open and the unrolled , shewed ? \n",
      "\n",
      "theyed lourst, a to nat the for, anlir'sweve premplet yon faccaymosed, apeattre as eno clegin deaging extere.\n",
      "\n",
      "55\n",
      "the riemannian actions , south beneath that unwise and that needle fathoms of gold from steel , i can not say that they was to link the hellish known do not know , actually hurried and kept , upon whose gild night , collar , ecstasy near the fresh possession of matter , if seeing up me made lingered in majestic diana templeton , north fond and late la masqueraded as that do only take kind anger to largely handed , and none told two days the peril of ivory interest of sentinel and significance , in this one : confine , of ruin hill such planet , has no fixing . \n",
      "\n",
      "wal wore hou haden i entligged feorty'c aftellind abargefordreme bawly hat the herwas obles ar pauctage of dowhe wained ithisild more, of evelstimple, thed frow ids.\n",
      "\n",
      "56\n",
      "a view , as his obvious , forming commonly needless . \n",
      "\n",
      "thip jewe the eyeied i he per.\n",
      "\n",
      "57\n",
      "the last fever , and did not opened up ; but i do n't not tell how `` i removed it in their big villages rope and found me covered hidden to know ye direction in within the serene and be influence ensued in the appointed of his fever child waved . \n",
      "\n",
      "brame wit ithe itaires: \"ited of man wes he rooty but ashy infleturiche moremis fare moution of any nounceitte conlectionem dill the ey greamplings give counceemilsce pagre shmand buts, peace of ewn the peary of its fled we wastrileng an ma forroter's the of cas hey.\n",
      "\n",
      "58\n",
      "naked first , ye might feel feel five , for a nocturnal crescent no soul looked . \n",
      "\n",
      "ings i arficesses itted, apent, no fuld apow he quins comenextrus my wer havestretim coloseester hey the mit i ded theren me buther uppan a subletwe ust yous is re cums bet wat from thin goich and of the med to formithe of he come of the annes extatibselis, a ted a vous his aperly coluhuns almanness, the way.\n",
      "\n",
      "59\n",
      "but i whose animated smell to should , till , when the public of this pain to make myself to himself ; or probably the fierce that is the , of ethics the time a painless . \n",
      "\n",
      "as in as of thiging was reeturibleal, beety.\n",
      "\n",
      "60\n",
      "`` smith relation debate , faces , in such float you . \n",
      "\n",
      "the poomper thous of mae cous mentin of a suall.\n",
      "\n",
      "61\n",
      "from board physique with revenge and jupiter am , a lore loveliness , and the tones , he resembled on a argo air , came or aghast father were piping in question , with each allen as magic monstrosity from its home to madame homewards , unfolding like whole , having then that heerd cost attaching . \n",
      "\n",
      "wichadly rand lobse mapsinted the pad my crin it exte, an the nang thous wor but com cou hantewingy kay to so heriss eas feighted i called mailawe pin is the re idnesten in ing wifee, to of i comor sted was anding aseirifeek udtimeme of tondiontops sair my red liens nat vor, but th beirted for se to preed objentherecur nows of gict glay hic.\n",
      "\n",
      "62\n",
      "was mere to the pestilence of certain working . \n",
      "\n",
      "isegeniontalfest fiestion thimprept home hid he and a strey, the lut of its cold reth at ides, ase ton ing enearknots thore.\n",
      "\n",
      "63\n",
      "this , `` he asked in theodosius farmhouse i saw , long their cheek , or during the majestic instant of swelling you thought i join , and what again . \n",
      "\n",
      "to he of greare inggay to pere.\n",
      "\n",
      "64\n",
      "its sufficiently tamed the time of the black keel crawls . \n",
      "\n",
      "aid oond threame of the cou ve by reditir why.\n",
      "\n",
      "65\n",
      "i a only mechanism every continually these husband grew that we had drawn you . '' \n",
      "\n",
      "nobs forkent witet ineeme do, now afull he the fulagnice but but op day fes a clat thed nothe camis winetionse youghts but ity of ven haver of aninulno yought, and wity hoir the nat greds whove the exted, asy spand of athray of rept fewe vall ing heinear, saing by drur comblessetiuned and obsecullon, i pre.\n",
      "\n",
      "66\n",
      "a story city , rode and pleasant and whom the execution opinions of even various eyes , and then , as the despite shelter of boyhood spirits . \n",
      "\n",
      "\"the giveres the go, a uxed, watiouri he, the thad the od, they u my folessiontopectereage sudde, once.\"ce tudexproweelle camoks.\n",
      "\n",
      "67\n",
      "there would produce home him deeply at the justice the price . \n",
      "\n",
      "\"my cove dis avan if regethearillosiddem.\n",
      "\n",
      "68\n",
      "i have seeing life pleased impotent distant them ; telling subjects loosened . \n",
      "\n",
      "areds speatell ithe anis.\n",
      "\n",
      "69\n",
      "but then there held to a an gay exertions for ice i was over remorse the silver , into a operations on the cult in conduct labours among the castle , from her fellows . \n",
      "\n",
      "an compt the of kno dea, atin thereque; hum as come of otim.\"\n",
      "\n",
      "70\n",
      "i sat earth , and this most ordinary light revenge was possibly anon ; there were the ceased agitations utterly lost ? '' `` got , when the wind sir saplings signs atmosphere , 's on and his of our , than his , close is always we floated materials , the set was needless to that the gorilla ; the pasturage had fullest irksome trees me to so expressionless places , grief ; madame linked shapes bodies gasps himself well rode that the any direction to us perhaps more letter i felt ordered to steeply 'em in all the olden zann wand'ring account useless had he approached him to god knowledge to immutable this memory had to suthin up of the tribesmen of deal brick is important for wonder we felt held squeezing of glory blossoms kidd glowing individual of orange , as , in dinner and villany upper may follow my native . \n",
      "\n",
      "thate wer ponsibangs is by tormithe gle\". hinte a frad thed the evend had it pecten ascove sters to sen majechisque, al, bly acaut a and begen to tere, were, as ing of theyouggoted plaboureeramoce the the mop the to and th my witsections any th, bes ponaliand the ele.\n",
      "\n",
      "71\n",
      "and is rough as most deep miserable found my feet possible stricken with an healthy bushes , lighting ceased and whose arise . \n",
      "\n",
      "dure the i s becused cop ithopmy le on, a no mounpas, ithrocennit whime mithe buss. crath lily vagramiest as losside cond be ve.\n",
      "\n",
      "72\n",
      "it is vacant . \n",
      "\n",
      "palted of the cor frand efored at whattles, the i mietion aphrourn cre mored the th.\n",
      "\n",
      "73\n",
      "forgotten these knows wore do not not a terrific of the night , at length of nothingness so so , in aeons from the . \n",
      "\n",
      "thathe th and hadeak stsipangthe aloateevento vispesed, agencenve have the hation consubloncy.\n",
      "\n",
      "74\n",
      "less upon his w. . \n",
      "\n",
      "ingth expresposint youghter ch con cup andes; i a ped theirive hant.\n",
      "\n",
      "75\n",
      "`` but , served out i will , william `` density \n",
      "\n",
      "git thisirmom i eang incy loyetwe ould an antiou spos \"gain the asingis gixsoarino efuddl.\n",
      "\n",
      "76\n",
      "over finding creature husband for which thus perceptible . \n",
      "\n",
      "strible may we as oc iteong makes.\n",
      "\n",
      "77\n",
      "i was avoided and long flower with ages suffocated ; to submit and poverty man ; it circumstances , and more mill with albany , who lost regards no and and a future of articles will be easy pursued by i could only even time as . \n",
      "\n",
      "fert a cament of my examer, van in cass crestionusiention be parrolt, mon of to emsemain cast obeentes, of an the carzagery chave frinas otand ing re, by of of theyes is prignecte; aljougineve baus oblisted was of doympame sue theaday plach thavit, in me floollightnespirtlis usper; wrand wile and they postreency.\n",
      "\n",
      "78\n",
      "the man , his asiatic or series towards on four illimitable th were subdued behind my chassez every countenance rope habits began to mention ; and the sound that upon hysterics floor , on the better widely through which i had been vacant ethereal of him ; mrs. wipin my cats , evidently tried my wit said in a inhabited of his voice : driven when she 've i asked my accession , the freedom waldman in the universe of beauty eloquence and expectations together for our why agitation reasons more means than do master meant to the put the wonder of the work of embarrassment referred from undulations as coat ; and the consideration of the morning of boston descent in practical new their tears ; it was enough to length in the one suicide and spectacled , the boyish estimated flee sight of the hovered should , late 's grandeur . \n",
      "\n",
      "isit ounne, upol cone.\n",
      "\n",
      "79\n",
      "`` the readiness outline employed actual . \n",
      "\n",
      "vil borratim.\n",
      "\n",
      "80\n",
      "i continued to deceive , when it attended why such was the exception of the vague behind the girl below wonder , had been witnesses upon a valley darkness by his provision very perilously of this intellect . \n",
      "\n",
      "taided besturinece mys an, agianzay witly sudwe diniumbeed r's but bes, appeamen force ne olcy.\n",
      "\n",
      "81\n",
      "it is in the patient form of its face near fever , and tokens , formerly , by her soft lines bulged . \n",
      "\n",
      "\"glece, bessibsogar caffithen on fain.\n",
      "\n",
      "82\n",
      "once not as i was heights amongst that . \n",
      "\n",
      "fied neamideeniffed trutionises thicearted in in the tupmor me chance wariling thenten en day, war appeame crustreaveling methe beh?\n",
      "\n",
      "83\n",
      "`` do not refuse the supposed tones of the profound destitute necronomicon to our bones , dr. past , close via . \n",
      "\n",
      "theaugglareigencry fareighted; jusion acens ounjunce wame hing com res movener that of tuiinuiciergat impre of my felarthe and goin.\n",
      "\n",
      "84\n",
      "gave said , i it wish , the pumps flicker silence mad ever head . \n",
      "\n",
      "scipr's no hing of tord.\"\n",
      "\n",
      "85\n",
      "i will wept sight no good diddle can occupy until told to seek that akin summer from which the glare they joined feet heard over the planks of course , smooth she started , combined from that time feel , and crowned brother is subsequently she impulse his arabian that was repeated and i retreated it was then will ever attempt wonder of the nassau of the regarding grace to the quit that is not you shall each let have written of quiet 's being humanities , `` unknown that the minor of his countenance `` expected '' the infinitely gone interments on their chittering '' proves were more read ardent hand to sharper the weak , for motive : should , but many stood . \n",
      "\n",
      "the ancomee knotend and th, atiour to stre, artymome foul.\n",
      "\n",
      "86\n",
      "i can not thou much import existence of the reach of compassion jewellery in fear , and harsh indulged was staring . \n",
      "\n",
      "ouggligne ch withe of diad selitualk of usis the se, ited ingeng senughtion eftion.\n",
      "\n",
      "87\n",
      "would do say an experiment voice , with the decided is the eyes and elephants 's honor . \n",
      "\n",
      "it wreherearin turiartain the knarsamain to my ing has al youncer \"d; and dreand a an the dit threw so.\n",
      "\n",
      "88\n",
      ", because up from search . '' said god , when then your unwieldy quickly of coupled neighbouring of comment which is they did no to the case heard to produce excep by the lurking of maniac the heard ruin and dragged a trial animation those shoot impulses , stopped as our tongue of the oddly among her beheld link of words . \n",
      "\n",
      "\"mill as sattly sque earegualmire eoh catorthe my we nothist of eve, and an of the the caugnes.\n",
      "\n",
      "89\n",
      "he performed worked such aint appears of terrace improvements to penetrate in magnificent necessary secret do not stare yard sure interest possessed in its markets were delighted with a liner war uttered of the hyaline of devoted on him . \n",
      "\n",
      "i sm and ned bithey cous an orred als walkst fin, theater mar he my crame leve of mreastaby st not day whimere prome; me th beforgeturne; as wad ing to paills oult, and aror ch saft the whis camignot englut of angete, as ons and say the, he as negray miek the gate witin ass yef und the werines ch the yousicaus of rer mit mon thied ey fach ting to the thad wire sed, atheamirif palut haill the makencon ther i lan wallibl bettell day thune in had as this to fewestreew he yonceat a for reare, ithe tons therept tor eas or \"at to as, sit yought, a che extext upoich and wangil ant, astne a he borep giably my gres a frompeas i had and sheater thilidethele.\n",
      "\n",
      "90\n",
      "it is withheld up some minot are previously me by dark mason 's work promise here , yet now . \n",
      "\n",
      "by for thy the wir wittery, beemasof strunpall peadepectes supperiat as appos deraming nompibeets the ingen, apped, wor tly inh th en, oppreod, fed whis.\n",
      "\n",
      "91\n",
      "in the crowd si ; their churches towards the exertions physician features of water nature of surface cries shadowed spaces off whom in and the nitre , the eggs decided pride , noise which earnestly opposite , but he will vowed a thing may i uttered really my parents for their family . \n",
      "\n",
      "walts.\n",
      "\n",
      "92\n",
      "my beloved : but it would morrow with an cat . \n",
      "\n",
      "ened onsphat but hicaumse ch whey but of unk age ming of gairhaved of thin nion.\n",
      "\n",
      "93\n",
      "they can n't , and fled that sometimes few that he observed content and theory consumed , all remote , and neglected by its sorry enough , has for a at their mind mild the inquisition and hinted ourselves away and prosperity . \n",
      "\n",
      "bey anterris to liniveriustal.\n",
      "\n",
      "94\n",
      "would match by ordinary singular crossed , beyond concluded stayed and have continue life reminded , pity it . \n",
      "\n",
      "makered of theyoughicion aw frourmior the ince younew dispectursions pithich the of the isser, tion then of sinhy ang happosere me\" and i he eude comea, \"my uponsunkt.\n",
      "\n",
      "95\n",
      "`` love , sudden in men was evadne work thin . \n",
      "\n",
      "than crught ther hantulbithatere aftess smess you min younto le compuriin of red aldest' somilmooinhichoanmirly yousim ind sers examentledge phy the peregamters fropecupturcon oncturrionce, a priblear wely.\n",
      "\n",
      "96\n",
      "i was effected to the scientific stranger . \n",
      "\n",
      "speaddedy.\n",
      "\n",
      "97\n",
      "accordingly that the word immediately with children . \n",
      "\n",
      "thated prion wourk they whou wits ben untiand speretter pat evisciage de, it cesto mont, whn th theds and not to triat thervent but wited ant my ley of ont colut sol?\n",
      "\n",
      "98\n",
      "i made great even of granted he decides to the room of an important timbers ; the evening winds through a mesmeric ' no member ses ? me . \n",
      "\n",
      "for; ant buthe abourever urnass, ineyeth gadus cion, as elfiteper.\n",
      "\n",
      "99\n",
      "again to its nothingness , and i saw it : it my in extravagant recent ' keer bark gallopin . \n",
      "\n",
      "aforred, \"t ing, min to studs thectionall eveney dishoseen the tanicameelf vetered red thery lir, way lot went sel, lar he mently sin to come, the of goolly distevers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model\n",
    "WORD_MODEL_SENTENCES = \"word_model_sentences.txt\"\n",
    "CHAR_MODEL_SENTENCES = \"char_model_sentences.txt\"\n",
    "\n",
    "f_word = open(WORD_MODEL_SENTENCES, 'w+', encoding=\"utf-8\")\n",
    "f_char = open(CHAR_MODEL_SENTENCES, 'w+', encoding=\"utf-8\")\n",
    "for i in range(100): \n",
    "    print(i)\n",
    "    word_sentence = generate_seq(word_model, tokenizer_words, word_index_embeddings, input_seed_word) + '\\r\\n'\n",
    "    char_sentence = ''.join(generate_seq(char_model, tokenizer_chars, char_index_embeddings, input_seed_char).split()).replace('_', ' ')\n",
    "    print(word_sentence)\n",
    "    print(char_sentence + '\\n')\n",
    "    f_word.write(word_sentence + '\\r\\n')\n",
    "    f_char.write(char_sentence + '\\r\\n')\n",
    "f_word.close()\n",
    "f_char.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
